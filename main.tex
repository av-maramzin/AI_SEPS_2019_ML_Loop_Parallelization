\documentclass[sigconf,10pt,review,anonymous]{acmart}

%\usepackage{booktabs} % For formal tables

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}

\graphicspath{ {./figures/} }
 
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA}
\acmYear{1997}
\copyrightyear{2016}

\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
\editor{Jennifer B. Sartor}
\editor{Theo D'Hondt}
\editor{Wolfgang De Meuter}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{A. Maramzin et al.}

\begin{document}

\title{Smart Manual Software Parallelisation Assistant}

\author{Aleksandr Maramzin, Christos Vasiladiotis, Murray Cole, Bj\"orn Franke}
\affiliation{%
  \institution{The University of Edinburgh}
  \streetaddress{Informatics Forum, 10 Crichton Street}
  \city{Edinburgh}
  \state{Scotland}
  \postcode{EH8 9AB}
}
\email{(s1736883,s1576261)@sms.ed.ac.uk, (mic, bfranke)@ed.ac.uk}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

\begin{abstract}
\quad Since automatically parallelizing compilers have failed to deliver significant performance improvements, programmers are still forced to parallelize legacy software manually for all but some niche domains. Rather than hoping for an elegant silver bullet, we acknowledge the role of a human expert in the parallelization process and develop a \textit{smart} parallelization assistant.\newline\null
\quad In its essence our assistant is yet another application of machine learning techniques to the field of optimizing compilers, which tries to predict the parallelisability property of program loops.
We use a version of NAS Parallel Benchmarks (NPB) hand-annotated with OpenMP parallelisation pragmas to train our model. We show that it is possible to achieve a good prediction accuracy of around 90\% for our problem using only static program features. We outperform all available baseline random predictors working at an accuracy ranging between 40\% and 70\%. To get a real practical application of our techniques, we integrate our trained ML model into 2 assistant schemes. Our schemes mitigate the effects of ineradicable statistical errors and make them not that critical. As a result we extend capabilities of Intel C/C++ compiler in the task of parallelism discovery and increase the amount of parallelism found in SNU NPB benchmarks from 81\% to 96\%. The second scheme is basically an assistant, which directs programmer efforts by pointing the loops, which are the most highly likely to be parallelisible and profitable as well. Thus, decreasing the efforts and time it takes to parallelize a program manually.

%
%Parallelising compilers comprise powerful program analyses, yet they fail to deliver a parallel speedup on real-world programs outside the domain of Fortran-style array based computations due to the limits of static dependence and profitability analyses. Decades of intensive research have not yet delivered a breakthrough, and we do not expect a major breakthrough in static analysis in the near future. In order to accelerate a software a programmer still has to manually analyse and parallelise it. At the same time, the field of machine learning (ML) has made a massive progress and ML techniques are, in fact, already used in parallelising compilers to support profitability analysis and scheduling decisions. In this work we apply ML to predict parallelisability property of program loops and to guide a human effort of software parallelisation.\newline\null
%\quad We use a version of NAS Parallel Benchmarks (NPB) annotated with OpenMP parallelisation pragmas to train our model to predict loop parallelisability property. We show that using only static program features we can achieve the average prediction accuracy of around 90\% against various baselines (random, most frequent, etc.) ranging from 40\% to 70\%. Inherent to all statistical methods false positives comprise around 6\% of cases. To eliminate the danger of false positives we require a final programmer approval and propose 2 schemes of providing a programmer with a parallelisation feedback. The first scheme enhances Intel C/C++ Compiler (ICC) parallelism discovery in SNU NAS benchmarks from 81\% to 96\% of all SNU NPB parallel loops. The second takes benchmark profiles and provides a programmer with a loop rankings to follow in benchmark parallelisation. Enhanced ranking allows a programmer to converge to the best achievable performance on a benchmark faster, than by following a mere profile based order.
%Here: Novel use of ML in parallelisation to guide human effort: Once all
%automatically parallelisable loops have been parallelised using state-of-the-art
%auto-parallelisers, ML predicts which loops to parallelise next. 
%Evaluated and outlook
%In this work we 
%All modern hardware is highly parallel, but in order to fully utilise availability of all these resources software must be parallel as well. There are numerous approaches to the task of software parallelisation ranging from manual approaches to fully automatic ones. In this work we investigate a relatively new approach to the task of software parallelisation: a machine learning assisted one. We use a version of NAS Parallel Benchmarks annotated with OpenMP parallelisation pragmas to train our model to predict loop parallelisability. We achieve 93\% generalised prediction accuracy with a baseline (random predictor) around 60\%. We study parallelisability of NAS benchmarks and its utilisation with Intel C/C++ Compiler (ICC) and propose a scheme, which might be used to supplement ICC with a ML-based tool. Proposed scheme increases ICC parallelisation coverage in the NAS Parallel Benchmarks from 86\% to 99\% and comes close to its algorithmic limit. After that we show, that for majority of NAS benchmarks this parallelisation coverage increase transforms into the actual performance improvements as well. We achieve an average speedup of 2.5x relative to the state-of-the-art parallelising Intel C/C++ compiler and come closer to 3.5x average speedup of the expert hand-parallelised version.

\end{abstract}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

\maketitle

\section{Introduction}

For several decades, parallelising compilers have been the subject of intensive academic research \cite{XXX} and industrial investment \cite{XXX}. Yet, for most real-world applications they fail to deliver parallel performance \cite{XXX}.


In this paper we take a different approach: We acknowledge the role of the human expert in the parallelisation process, and develop a machine learning based assistant guiding the users to focus their resources on those loops most likely to result in a positive return-of-investment (ROI). We do not aim to completely automate parallelisation including program analysis and transformation, but instead assist the user by suggesting an ordered ranking of loops for manual parallelisation, where this ordering considers both the estimated effort and profit of parallelising each loop in turn.

\subsection{Motivating Example}

\subsection{Contributions}

\subsection{Overview}

\subsection{Paper structure}
\quad The most substantial part of this work was aimed at the creation of machine learning (ML) based model, capable of accurate prediction of loop parallelisability property. Section \ref{predicting_parallel_loops} describes all the details of machine learning part including the features we chose and the exact training/testing methodology we used. But even close to perfect prediction accuracy is not enough, if we cannot find a real practical application to utilise our predictor. Section \ref{practical_applications} proposes 2 complementary schemes we might integrate our predictor into. The first scheme helps Intel ICC compiler to discover additional parallelism. The second scheme is basically software manual parallelisation assistant working with a program profile and directing human efforts at the most likely to be parallelisible (and profitable as well) loops. Section \ref{evaluation} presents a quantitative report on our work.  

\section{Background}

\subsection{Parallelism Discovery}

\subsection{Profitability Analysis}

For example, SUIF \cite{Wilson:1994:SIR:193209.193217} uses a simple heuristic based on the product of statements per loop iteration and number of loop iterations to decide whether a parallelizable loop should be scheduled to be executed in parallel. In contrast, \cite{Tournavitis:2009:THA:1542476.1542496} uses a machine learning based heuristic, which consumes \textit{dynamic} program features collected in a separate profiling stage, to decide if and how a potentially parallel loop should be scheduled across multiple processors.

\subsection{Machine Learning}

\subsubsection{Feature Engineering in ML}
\quad In order to learn loop parallelisability property using machine learning (ML) algorithms, we need to characterise loops with a set of quantitative metrics (features in ML terminology). In other words, we need to quantify loop parallelisability property. The process of finding these metrics is called feature engineering.\newline
\quad We base our feature engineering efforts on the vast body of work done in the field of optimising compilers. Particularly in the domain of program dependence analysis \cite{Kennedy:2001:OCM:502981}. We build Program Dependence Graphs (PDG) \cite{Ferrante:1987:PDG:24039.24041} for program loops, perform generalised loop iterator recognition analysis \cite{Manilov:2018:GPI:3178372.3179511} and then based on that we compute our quantitative loop features. All that work is being done withing the LLVM compilation framework and uses LLVM Intermediate Representation (IR) for all ML feature computations. We dump PDGs in a dot format and systematically use these visualisations in our work in order to debug and decide on the best features to use.
\subsubsection{Supervised Loop Classification Labels}
\quad We use Seoul National University (SNU) \cite{snu-npb-benchmarks} implementation of NAS Parallel Benchmarks (NPB) \cite{nasa-parallel-benchmarks} in our research. These benchmarks have been manually parallelised by the developers using OpenMP framework. This gives us an expert opinion on the parallelisability property of SNU NPB loops. In addition, we parallelise SNU NPB benchmarks with Intel C/C++ Compiler \cite{icc-compiler} and get its optimisation reports. We parse ICC reports in order to extract parallelisation/vectorisation information relevant to us and add it on top of the information provided by OpenMP pragmas. We study the loops of SNU NPB benchmarks manually and add obtained insights into our feature engineering and loop classification efforts.\newline
\quad The definitions of all devised and implemented features as well as the details of all described above are discussed in the section \label{predicting_parallel_loops} of this work.  

\section{Predicting Parallel Loops}
\label{predicting_parallel_loops}
\quad This section describes the exact way we applied machine learning (ML) techniques to the problem of loop parallelisability prediction. Here we describe the features we chose to represent program loops, as well as the whole ML training/testing methodology we used.\newline\null
\quad Our features and the exact parameters of methodology (automatic feature selection methods, ML models and their hyper-parameter spaces, etc.) have been iteratively tuned and refined with the help of K-fold CV. Predictive accuracy, recall and precision scores were used as the main selection criteria. To get the most accurate and honest assessment of our ML models we kept the testing data hidden and used only the training data during all ML pipeline stages. Following subsections present the final results rather than the path towards them.

\subsection{Overview}
\label{ml_overview}
\quad\textbf{Supervised ML Classification Problem}: Create a machine learning (ML) model and train it to classify loops of Seoul National University implementation \cite{snu-npb-benchmarks} of NAS Parallel Benchmarks \cite{nasa-parallel-benchmarks} as parallelizable or not.\newline\null 
\quad For any machine learning methodology to work and make accurate predictions we need to choose and set all its stages correctly. Apart from ML model selection (Support Vector Machines, Decision Trees, Neural Networks, etc.) we need to provide training and testing data sets in the right format and decide on the exact training/testing approach we are going to employ.\newline\null
\quad We used facilities of \textit{scikit-learn} \cite{scikit-learn} Python library for all machine learning related tasks. We developed a scripting framework based on this library taking an input data (training and testing) along with a ML pipeline configuration INI file. Configuration file allows for a flexible change in the settings of an experiment (ML model to use, its hyper-parameters, the exact automatic feature selection methods, etc.). The following sections give a detailed description of all ML pipeline stages.\newline\null

\subsection{Loop Analysis \& Features}
\label{loop_analysis_and_features}
\quad The one of the most important parts in applying machine learning to a given problem is the task of feature engineering. In our loop parallelisability classification problem we need to pick the right features of program loops, which are the most reflective of loop parallelisability property. In the task of coming up with a set of loop features we are guided by general program dependence analysis theory \cite{Kennedy:2001:OCM:502981}, exact types of loops present in SNU NPB benchmarks and Intel C/C++ compiler optimisation reports.\newline\null
\quad There is a range of SNU NPB loops, which escape Intel compiler parallelisation for different reasons: indirect array references, unrecognised reductions on array locations, pointers with statically unknown memory locations, etc. But all that range of different reasons is going to ultimately materialise into data and control dependencies present between loop instructions, represented as edges on the Program Dependence Graph (PDG) \cite{Ferrante:1987:PDG:24039.24041} of a loop. Figure \ref{fig:pdg} shows an example of a PDG built for a simple loop and visualised with our tool.\newline\null

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/pdg_example.pdf}
\caption{Example of PDG of a simple loop with a cross-iteration dependency built and visualised by our tool. These graphs were used during tool debugging and feature engineering processes.}
\label{fig:pdg}
\end{figure}

\quad The PDG graph consists of LLVM IR instructions as nodes and different sorts of dependencies between them as graph edges. Dependence relation lies at the the very essence of loop parallelisation, on the other hand just counting the number of dependence edges in the PDG of a loop is not enough to make decisions about loop parallelisability. Dependence relations might exist only withing one loop iteration or span across multiple, thus preventing parallelisation.\newline\null
\quad To refine our features we use the work on generalised loop iterator recognition \cite{Manilov:2018:GPI:3178372.3179511}. Generalised iterator recognition analysis separates loop iterator from the actual payload providing us with finer loop partitions to base our features on. As work \cite{Manilov:2018:GPI:3178372.3179511} explains, loop iterator is a strongly connected component (SCC) on the PDG with no incoming dependence edges. There are SCCs in the payload as well. Usually they consist of just 1 instruction, but when we have a cross-iteration dependency they tend to grow larger and form a cycle. We call such SCCs \textit{critical}, as ones preventing parallelisation. Figure \ref{fig:pdg} highlights both the iterator and the critical SCC present in the example loop. Inner loop iterators tend to appear as critical SCCs for the outer loop as well. To separate these cases we use \textit{inner loops number} and \textit{loop depth} as separate ML features.\newline\null  
\quad The feature engineering process has been conducted iteratively and has been guided by the change in ML models predictive performance with the addition of one feature or the other. This process involved several methods. First, we tried to capture in our features the differences between PDG visualisations for parallel and non-parallel loops. Then we studied the source code of SNU NPB benchmarks along with ICC optimisation reports and tried to understand why ICC failed to parallelise some of SNU NPB loops and transfer those insights into reflective features.\newline\null
\quad We ended up with a set of 74 static loop features, which are based on the structural properties of PDG and the types of instructions constituting them. Table \ref{tab:loop_features} summarises the main groups of devised features.\newline\null
\quad Many of our features have a simple and intuitive motivations behind them. Loop proportion related features are backed up by the the fact, that bigger loops are harder to parallelise. Big iterators contain complex cross-iteration transitions (e.g linked-list update), unknown iteration numbers, etc. Critical SCCs limit loop parallelisation further. Cohesion features do not have an apparent intuition. They characterise how tightly components of loops are coupled together in terms of the number of edges between them. Loop dependencies number features count the number of edges in different loop parts as well as their types. Loop instructions nature characterise the types of loop instructions, assigning more importance to memory reads/writes, calls, branches. Uninlined funtion calls usually prevent loop parallelisation. Intensive memory work (memory read/write fraction features) complicates parallelisation as well.   

\begin{table*}[h!]
    \centering
    \begin{tabular}[c]{|p{2.5cm}|p{4.0cm}|p{9.0cm}|}
        \hline
        feature groups & features & description\\
        \hline
        \multirow{3}{2.5cm}{Loop Proportions} & Absolute Size & the number of LLVM IR instructions\\\cline{2-3}
        & Payload Fraction & $\frac{payload instructions number}{whole loop instructions number}$\\\cline{2-3}
        & Proper SCCs Number & number of payload SCCs with more than 1 instruction\\\cline{2-3}
        \hline
        Loop Dependencies & \multicolumn{2}{|p{13.0cm}|}{The number of PDG edges corresponding to different dependence classes: read/write order (\textbf{True}, \textbf{Anti}, \textbf{Output}), dependency media (\textbf{Register}, \textbf{Memory}, \textbf{Control}), other (\textbf{Confused}, \textbf{Cross-Iteration})} \\
        \hline
        \multirow{2}{2.5cm}{Loop Cohesion} & Iterator/Payload & $\frac{Num of Edges Between Iterator/Payload}{Total Loop Edges Num}$\\\cline{2-3}
        & Critical/Regular Payload & $\frac{Num of Edges Between Critical/Regular Payload}{Total Loop Payload Edges Num}$\\
        \hline      
        Loop Nature & \multicolumn{2}{|p{13.0cm}|}{The number of different kinds of critical to loop parallelisability instructions (memory reads/writes, calls, branches, etc.). These numbers are computed for different loop parts (iterator, payload, critical payload)} \\
        \hline

    \end{tabular}
    \caption{The set of static code features devised to capture loop parallelisability property.}
    \label{tab:loop_features}
\end{table*}

\subsection{Feature Extraction}
\label{feature_extraction}
\quad To extract all devised loop features we developed a tool, based on the LLVM compiler infrastructure \cite{Lattner:2004:LCF:977395.977673}, \cite{llvm-compiler-infrastructure}. The tool is basically a set of LLVM function passes working on the SSA-based LLVM IR and can be found on the GitHub \cite{github-ppar-tool}.\newline\null
\quad There are several groups of inter-dependent passes. Once we have an SSA-based LLVM IR we build data, memory and control dependence graphs (DDG, MDG, CDG) of program loops and combine them into a single PDG. Then we run a search of strongly-connected components (SCCs) on these graphs and generalised iterator recognition analysis \cite{Manilov:2018:GPI:3178372.3179511}. Once all parts are ready: dependence graphs are built, their iterators and payloads are identified we run a set of feature computing passes. At the end we dump all our computed features into a table to be later fed into scikit-learn based ML scripts.\newline\null

\subsection{Feature Selection}
\label{feature_selection}
\quad To do a feature engineering task and find a set of features, capturing the loop parallelisability property we used our domain knowledge and different case studies. To improve our feature selection further and refine it empirically we additionally employed a sequence of automatic feature selection methods. Doing so also aligns our methodology with a standard accepted one.\newline\null
\quad In our case we have 74 features, many of which are just slightly different variations of the same property. In order to decrease the chances of over-fitting our ML model we need to discard some features, which are redundant or irrelevant.\newline\null
\quad Our ML pipeline scripts can be configured to apply an arbitrary sequence of different feature selection methods to the provided data set. We use feature selection methods available in the scikit-learn python library. We use comparable feature selection pipelines for all the models we train. First we filter out all features with a low variance score, then we fit a decision tree based model and select all features with importance score above the threshold. Then we repeatedly run recursive feature elimination by the cross-validation (RFECV) in an attempt to improve several targets: recall, precision and accuracy scores. Table \ref{tab:best_features} illustrates the relative ranking of the 10 highest scoring features in our automatic feature selection runs.\newline\null  
\quad SNU NPB benchmarks contain a lot of uninlined function calls and it is unsurprising that the amount of call instructions in the payload of a loop ranks the highest. Despite the absence of straightforward intuition behind cohesion metrics, they tend to correlate with parallelisation labels well. Loops heavy on memory writes also significantly affect the parallelisability property.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{5.5cm}|p{1.5cm}|}
        \hline
        feature & importance\\
        \hline
        payload call fraction & 23.5\\
        \hline
        payload mem dep num & 4.0\\
        \hline
        iter/payload non-cf cohesion & 18.5\\
        \hline
        iter/payload total cohesion & 3.2\\
        \hline
        payload mem write fraction & 6.1\\
        \hline
        critical payload non-cf cohesion & 2.9\\
        \hline
        loop absolute size & 5.7\\
        \hline
        payload getelemptr fraction & 2.7\\
        \hline
        critical payload getelemptr count & 5.3\\ 
        \hline
        critical payload total cohesion & 2.6\\
        \hline
    \end{tabular}
    \caption{Relative importance of our features, ranked by fitting tree based ML model.}
    \label{tab:best_features}
\end{table}

\subsection{Model Selection}
\label{model_selection}
\quad We use several machine learning algorithms available in the scikit-learn library to compare and find the best one. Among these are tree-based methods like (decision trees (DT), random forests (RFC), boosted decision trees (AdaBoost)), support vector machines (SVC) and neural network based multi-layer perceptron (MLP). Section \ref{evaluation_kfold} shows, that all these models perform comparably good with SVC and MLP being slightly better. 

\subsection{Model Hyper-Parameter Selection}
\label{model_hyper_parameter_selection}
\quad For any chosen parametric machine learning model we need to pick the right set of model hyper-parameters. Hyper-parameters are parameters that are not directly learnt within models. Typical examples include C, kernel and gamma for Support Vector Classifiers (SVC), exact architecture for neural network based models, etc. For any given model we use exhaustive hyper-parameter grid search and pick the node of the grid with the best cross validation score on the training set. Table presents hyper-parameter spaces we search for different models.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{2.0cm}|p{5.0cm}|}
        \hline
        model & hyper-parameter space\\
        \hline
        SVC & \textbf{kernel}: rbf,sigmoid,poly, \textbf{C}: 1,10,$10^2$,$10^3$ \textbf{$\gamma$}: 1,$10^{-1}$,$10^{-2}$,$10^{-3}$,$10^{-4}$\\
        \hline
        DT RFC AdaBoost & \textbf{n\_estimators}: 1,2,5,10,50,100 \textbf{max\_depth}: 1,3,5,7,10,15,20,30,50 \textbf{min\_samples\_split}: 0.05,0.1,0.2,0.5,0.7,0.9 \textbf{min\_samples\_leaf}: 1,5,10,15,30,50,60,70,100 \textbf{max\_features}: 1,5,10,15,30,50,70\\
        \hline
        MLP & \textbf{activation}: logistic,relu,tanh \textbf{solver}: lbfgs,sgd,adam \textbf{hidden\_layes\_sizes}: (5;5),(10),(10;10),(10;5),(5;2),(2;5) \textbf{$\alpha$}: $10^2$,10,1,$10^{-1}$,$10^{-2}$\\
        \hline
    \end{tabular}
    \caption{Hyper-parameter spaces to search for an optimal point for different ML models.}
    \label{tab:hyper_param_space}
\end{table}

\quad Support Vector Classifier model tends to vary parameters \textit{C} and $\gamma$, but almost always chooses RBF kernel. Multi-layer perceptron works the best with \textit{relu} activation function, \textit{lbfgs} solver and varying $\alpha$ and network configuration. Tree based methods tend to keep to relatively small depths (5-15).    

\begin{comment}
Model hyper-parameter tuning process consists of several parts. First, we determine the most important hyper-parameters for each ML model being used. Then, we build a grid with these hyper-parameter ranges. To assess each combination of hyper-parameter values, represented as a point on the grid we perform a standard process. We split our entire set of SNU NAS loops into training and testing subsets. We then take the training subset and further partition it into K different folds. For each fold we train the model with a set hyper-parameters on the remaining K-1 folds and test it on the chosen fold. We average calculated prediction accuracy scores and pick the best one for each hyper-parameter grid point. Then we take the combination of hyper-parameters, which corresponds to the best score.\newline\null
\quad Once the best performing set of hyper-parameters is chosen using CV on the training set, we train ML model with these hyper-parameters on the whole training set and subsequently test it on the testing set chosen at the very beginning.
\end{comment}

\subsection{Loop Classification Labels}
\label{loop_classification_labels}
\quad In order to derive our loop parallelisability classification labels we used several sources of information. First, we used SNU version \cite{snu-npb-benchmarks} of NAS Parallel Benchmarks (NPB) \cite{nasa-parallel-benchmarks} hand-annotated with OpenMP pragmas. There are a total of 211 OpenMP pragmas in all 10 SNU NPB benchmarks. These OpenMP source code annotations approximate the best SNU NPB parallelisation done by the original benchmark developers.\newline\null
\quad If we only use these labels for classification, we get a highly imbalanced data set. 
Moreover, not all SNU NPB parallelisible loops have been annotated with OpenMP pragmas due to considerations of profitability. Since we are using static program features reflectin algorithmic dependence-based parallelisability property we are risking to mislead our ML algorithm. To overcome these problems we use optimization reports of the Intel compiler as a second source of information.\newline\null
\quad To extract loop parallelisability labels from the Intel compiler's optimization reports we developed an optimization report parser \cite{github-icc-parser}. That task presented us with a number of technical challenges. Before ICC can actually parallelize or vectorize a loop, it applies a number of enabling loop transformations such as loop interchange, distribution, tiling, etc. The detailed description of all these transformations can be found in the paper \cite{Bacon:1994:CTH:197405.197406}. Applied to a loop nest, these optimizations might significantly restructure and distribute the parts of a loop across ICC optimization report. Moreover, ICC might parallelise only certain parts of transformed loop. At the end we considered a loop to be parallelisible by the ICC compiler if the latter hasn't found any dependencies and either vectorized or parallelized it. In the case of distributed loops, all parts must be parallelisible for an original loop to be considered as such. For a final correctness we conducted a manual verification on top of automatically extracted results.\newline\null
\quad Table \ref{tab:icc_stats} presents a parsing report, which summarizes the number of times ICC applied a certain optimization. The major cells are \textit{parallel} and \textit{icc}, which report the total number of truly parallelisible loops and the number of loops parallelised by ICC. As it can be seen ICC dompiler does not exploit all the parallelism available in SNU NPB benchmarks. Section \ref{evaluation_icc_competition} presents a study of reasons ICC fails to parallelise certain loops. 

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
        \hline
        total loops & 1415 & parallelised & 653\\
        \hline
        parallel & 995 & vectorized & 737\\
        \hline
        icc & 812 & parallel deps & 535\\
        \hline
        openmp & 210 & vector deps & 266\\
        \hline
        distrs & 34 & fusions & 214\\
        \hline
        collapses & 58 & tilings & 27\\
        \hline
    \end{tabular}
    \caption{SNU NPB ICC and OpenMP parallelisation statistics. The true parallel labels are denoted by \textit{parallel}. Number of loops parallelised by ICC is \textit{icc}. Remaining cells report on different kinds of optimizations done and reported by the ICC.}
    \label{tab:icc_stats}
\end{table}

\subsection{Training/Testing Methodologies}
\label{train_test_methodologies}
\quad In our work we use K-fold and modified Leave-One-Out cross-validation (LOOCV) methodologies to train and test our ML models. Detailed description of these methods can be found in the book on statistical learning \cite{James:2014:ISL:2517747}.\newline\null
\quad K-fold CV method blends all the loops from all SNU NPB benchmarks together in the single set and divides it into K equally sized splits. After that the method uses all the possible combinations of K-1 splits to train a model and test it against the one remaining split. Resulting accuracies are averaged to produce a final score. The advantage of that method in the context of our task is that it uses loops from all SNU NPB benchmarks for training and testing. When all our benchmarks differ in their, we do not miss any of them in our training process. That leads us to a better scores overall. For that reason we use that method to tune our model and report its true performance.\newline\null
\quad If we want to utilise our ML models in different practical scenarios (see section \ref{practical_applications}) we have to use separate SNU NPB benchmarks as a whole during model testing. To accomplish that we employ a modified LOOCV method to estimate the predictive performance our models can achieve against separate benchmarks in the set. Here we take all loops in every single benchmark as a testing set and train the model on all loops of 9 remaining benchmarks. The disadvantage of that method is that we exclude the whole benchmark out of the training process. If benchmark has a different nature from the ones we used to train the model, thim may come at the price of reduced accuracy.   

\section{Predictor Practical Applications}
\label{practical_applications}
\quad Once we have successfully trained our ML models to predict the parallelisability property of program loops, we need to find a way to utilise our predictors in real practical scenarios. Due to statistical nature inherent to all machine learning techniques it is impossible to completely eliminate all prediction errors. While false negative mispredictions might just miss available parallelisation opportunities and lose some performance, false positive mispredictions can break the program and are the most critical in the context of our ML problem.\newline\null
\quad For that reason we propose two schemes that leave the question of final parallelisation up to a programmer to decide. Moreover, to be useful our schemes do not even require a close to perfect predictor accuracy. The first scheme puts our predictor in the same harness with the Intel compiler and assigns the former a supplementary role to perform. As we show in the section \ref{evaluation}, it increases the amount of parallelism discovered in a program. Discovered parallelism does not necessarily materialises into real performance gains. To accelerate a program parallelisable loop has to be profitable. To take loop profitability into account we propose a second scheme, which takes the profile of a program and produces a loop ranking. This ranking is better than just a mere profile-based loop running time order in that it filters non-parallel loops out and moves them to the back of the ranking.

\subsubsection{Use Scenario 1}
\label{application_icc_competition}
\quad Figure \ref{fig:icc_competition_scheme} demonstrates the first application of our loop parallelisability predictor. That scheme is basically a tool, extending parallelism discovery capabilities of ICC compiler. We do not aim for a complete automation here. It is a separate and independent task to actually map all discovered parallelism onto the exact target platform and is left to a programmer. Yet another limitation of the scheme is that it doesn't consider the profitability of potential loop parallelisations.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/icc_competition_scheme.pdf}
\caption{First predictor application. Smart parallelisation adviser. }
\label{fig:icc_competition_scheme}
\end{figure}

\quad Table \ref{tab:combinations_table} summarises all the possible outputs our scheme might potentially produce. The most interesting cases here are the cases, when ML predictor disagrees with ICC compiler. If predictor says "yes, parallelisible" and ICC compiler says the opposite we might either discover a new parallelisation opportunity (case 011), or make a critical false positive mispredicton (case 010). In the 101 case ICC compiler "shields" ML predictor from making a false negative misprediction and losing performance. When feedback components agree with each other we do not gain anything over single ICC scheme. They might both say "no" to a truly parallelisible loop and thus miss some opportunities. Section \ref{evaluation} provides all quantitative characteristics and distributions of the scheme. 

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{1.0cm}|p{1.5cm}|p{1.5cm}|p{2cm}|}
        \hline
        icc & ML predictor & true parallel & classification bucket \\
        \hline
        0 & 0 & 0 & correct 0 (no) prediction \\
        \hline
        0 & 0 & 1 & missed opportunity \\
        \hline
        0 & 1 & 0 & false positive \\
        \hline
        0 & 1 & 1 & discovery \\        
        \hline
        1 & 0 & 0 & impossible \\
        \hline
        1 & 0 & 1 & icc shielding \\
        \hline
        1 & 1 & 0 & impossible \\
        \hline
        1 & 1 & 1 & correct 1 (yes) prediction \\  
        \hline
    \end{tabular}
    \caption{All the possible ICC competition scheme combinations}
    \label{tab:combinations_table}
\end{table}

%\quad In these scheme we propose a machine learning based tool, which can be used to speed the process of software parallelisation by pointing out the "right" loops for a programmer to direct his/her efforts. The tool takes several inputs. First input is the configuration of ML pipeline to use (ML model, ML feature selection techniques, etc.). Second, we need a set of programs, which can be used to train our ML model to recognise parallelisable loops. In our training scheme we propose to use static program features only (although dynamic features can be added as well). Such a scheme captures static properties (dependencies, sizes, instructions, etc.) of loops and thus their algorithmic parallelisability, but does not take into consideration the actual profitability of parallelisation of such loops. A loop can be parallelisible, but a lightweight one, without consuming much a a program running time. Not only the parallelisation of such loops seems meaningless, but can even slow the program down. To deal with that problem our tool takes the profile of the application the tool is used on. takes a set of programs for a training of ML model.    

\subsubsection{Use Scenario 2}
\label{application_profile_filter}
\quad The second scheme we propose takes an application profile into account. We collect an application profile with the help of ICC compiler. ICC instruments SNU NPB loops to measure their running time and outputs a runtime ordered list. This is not the best order, as far as loop parallelisation is concerned. There might be long running non-parallellisable loops standing at the biginning of the ranking. Ideal loop ranking would put all parallelilasible loops at the front and order them by their running time. All non-parallelisible loops should be at the back (their relative order does not matter that much, since we are not going to parallelise them anyway).\newline\null
\quad Our tool tries to come to that kind of ranking as close as it can. Fot that purpose we no longer consider 0 and 1 classification labels but extract internal probabilities from our ML models. We multiply loop running times on the shifted sigmoid function of probability that loop is parallelisible. Figure \ref{fig:sigmoid_3d} plots the function. As we can see from the plot, our function amplifies probabilities above 50\% and diminishes probabilities below $\frac{1}{2}$ line. At the same time low probabilities are still non zero. Such function shape protects us from filtering long running false negatives out of the ranking.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/sigmoid_3d.pdf}
\caption{Filter function used in our assistant. We used a slightly modified for our purposes version of a well known sigmoid (logistic) function.}
\label{fig:sigmoid_3d}
\end{figure}

\begin{figure*}[h]
\includegraphics[width=\textwidth]{figures/ft_ranking.pdf}
\caption{Loop ranking reordering with out assistant. }
\label{fig:ft_loop_ranking}
\end{figure*}

\section{Evaluation}
\label{evaluation}
\quad Sections \ref{predicting_parallel_loops} and \ref{practical_applications} describe our loop parallelisability learning methodology and its practical applications conceptually. In this section we report our results quantitatively.\newline\null
\quad Program running time on a real system is the most important characteristic, as far as software parallelisation is concerned. Thus, at the end of a day we aim to assess our techniques with running time used as the major criterion. In this section we also provide all reports on the predictive performance of our models, assessed by the means of K-fold CV and modified LOOCV techniques (see sections \ref{evaluation_kfold} and \ref{evaluation_loocv}). Section \ref{evaluation_icc_competition} reports on our ICC competition scheme (see section \ref{application_icc_competition}). At the end we show, that our smart assistant (see \ref{application_profile_filter}) can improve a straightforward profile based parallelisation and converge to performance levels of the best parallel version faster (see section \ref{evaluation_performance}).

\subsection{Experimental Methodology}
\label{evaluation_methodology}
\quad Both of our practical application schemes (see section \ref{practical_applications}) contain a trained loop parallelisability predictor as an integral part. Thus, our primary task is to set machine learning methodology and achieve a reasonably good prediction accuracy.
\quad Supervised ML task starts with labelled data. To get the data set we are going to use for ML model training and testing, we compile SNU NPB benchmark sources with Clang C/C++ compiler to the LLVM IR and run our LLVM opt passes on it. As a result we get a list of all SNU NPB loops with their computed feature vectors (see section \ref{loop_analysis_and_features}). To get the ground truth parallelisability labels (see section \ref{loop_classification_labels}) we independently compile SNU NPB sources with Intel ICC compiler and collect all its optimization reports. After that we parse ICC optimization reports to get a list of all loops and their labels. We intersect the list with OpenMP labels and do some manual verification. We combine all loop features and labels in the final table.
\quad To evaluate the predictive performance of our ML models we use K-fold CV technique on the collected data. The same technique is used to select the exact configuration of our training/testing ML pipeline. Once all methodology parameters are selected we measure the final predictive performance.\newline\null
\quad To apply our assistant scheme to SNU NPB we have to use modified LOOCV in a following way. We select a benchmark we want to parallelise with the help of our assistant. We train our assistant on the 9 remaining SNU NPB benchmarks. Then our assistant gives predictions for loops of selected benchmark. The details about the training process can be found in subsenctions \ref{evaluation_kfold} and \ref{evaluation_loocv}.

\begin{comment}
\quad Once we have gathered all the data, we start the oracle training process itself. First, we need to configure the exact stages of our ML pipeline for every ML model we are going to use in oracle training. This includes the selection of exact data preprocessing, automatic feature selection methods and model hyper-parameter search space. We perform that selection and its further tuning with the help of K-Fold cross validation (CV) method. By repeatedly doing model fitting and testing on the folds of the whole data set we pick the best ML pipeline configuration. This configuration does not change. Table illustrates the chosen ML pipeline configurations for different ML model. Table reports on the performance of Kfold CV.\newline
\quad After we get the exact ML pipeline configurations for all the ML models we use modified Leave-One-Out-CV in order to train and test our oracle on the different SNU NAS benchmarks, get benchmark loop parallelisability feedback and use it for benchmark parallelisation. We have 10 SNU NAS benchmarks. We choose one of them and run the oracle training pipeline on the 9 remaining ones. Once the oracle is trained we test it using the chosen unseen benchmark and get parallelisability feedback. Table reports on the performance of out LOOCV method. 
\end{comment}

\subsection{K-Fold Cross Validation}
\label{evaluation_kfold}
\quad In our work we use k-fold cross-validation (CV) method on the whole set of all SNU NPB loops to conduct a general assessment of our ML model predictive performance and the tuning of ML pipeline parameters, such as the exact sequence of feature selection stages and hyper-parameter space to search for every model. We repeatedly ran k-fold CV train/test method with varying ML pipeline parameters and measured predictive performance to get a rough estimate of optimal MP pipeline configuration. Figure \ref{fig:accuracy} plots prediction accuracy as a function of K for a ML pipeline configuration we chose to use in our experiments.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/prediction_accuracy_kfold}
\caption{Prediction accuracy measured using k-fold CV on the whole SNU NPB loop set}
\label{fig:accuracy}
\end{figure}

The figure shows that predictive accuracy is relatively consistent across different data set splits. Recall and precision scores exhibit the same stability across varying k values. 

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
        \hline
        ML model & accuracy & recall & precision \\
        \hline
        constant & 70.32 & 100 & 70.32\\
        \hline
        uniform & 46.27 & 41.50 & 69.79\\
        \hline
        SVC & 90.04 & 95.24 & 91.06 \\
        \hline
        AdaBoost & 86.96 & 92.92 & 89.06 \\
        \hline
        DT & 84.36 & 89.57 & 87.90 \\
        \hline
        RFC & 86.65 & 93.22 & 88.47 \\
        \hline
        MLP & 89.40 & 93.77 & 91.39 \\
        \hline
    \end{tabular}
    \caption{Average predictive performance for different ML models measured with a k-fold CV method on the whole set of 1415 SNU NPB loops.}
    \label{tab:average_accuracy}
\end{table}

\quad Table \ref{tab:average_accuracy} shows the average predictive performance for different ML models. Support Vector Classifier (SVC) model has the highest accuracy and has successfully managed to recall 95,25\% of all parallel loops. Table \ref{tab:icc_stats} shows, that ICC discovers 812 out of 995 parallelisible loops. SVC extends ICC parallelisation capabilities to 945 loops. Despite relatively high precision of 91.06\%, that extension comes at the price of some misprediction errors. Figure \ref{fig:prediction_stats} shows that unsafe (false positive) mispredictions dominate at 65.47\% of cases. Thus we need to devise a scheme, that will protect us and make these errors not that critical.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/prediction_stats.pdf}
\caption{Mispredictions nature.}
\label{fig:prediction_stats}
\end{figure}

\subsection{LOOCV SNU NAS Performance}
\label{evaluation_loocv}
\quad While k-fold cross-validation method mixes loops from all 10 SNU NPB benchmarks and provides a good statistical assessment of predictive performance on the whole SNU NPB data set generally, in order to use our predictor with the proposed schemes we need to change our methodology. For that purpose we use modified Leave-One-Out cross-validation (LOOCV) technique. We train our model on 9 benchmarks and test it on the remaining one. Doing so allows us to actually parallelise all correctly predicted parallel loops and get performance numbers for the benchmark being tested.

\begin{table*}[t!]
    \centering
    \begin{tabular}[c]{|p{1.5cm}|p{1.0cm}|p{1.5cm}|p{1.3cm}|p{1.0cm}|p{1.5cm}|p{1.3cm}|p{1.0cm}|p{1.5cm}|p{1.3cm}|}
        \hline
        benchmark & \multicolumn{3}{c}{average per model} \vline &	\multicolumn{3}{c}{most frequent} \vline & \multicolumn{3}{c}{uniform} \vline \\
        \hline
	    accuracy & recall &	precision &	accuracy & recall &	precision &	accuracy &	recall	& precision & accuracy \\
	    \hline
        BT & 85.7 & 92.6 & 90.0 & 78.4 & 100.0 & 78.4 & 51.9 & 48.3 & 83.3 \\
        \hline
        CG & 74.2 & 72.4 & 85.7	& 64.4 & 100.0 & 64.4 & 46.7 & 37.9 & 64.7 \\
        \hline
        DC & 72.8 & 71.0 & 51.5	& 29.0 & 100.0 & 29.0 & 55.0 & 37.9 & 28.9 \\
        \hline
        EP & 92.0 &	86.6 & 100.0 & 60.0 & 100.0 & 60.0 & 40.0 & 33.3 & 50.0 \\
        \hline
        FT & 56.5 &	37.9 & 84.4	& 63.0 & 100.0 & 63.0 & 43.5 & 34.5	& 58.8 \\
        \hline
        IS & 74.0 &	97.5 & 61.3	& 40.0 & 100.0 & 40.0 &	45.0 & 37.5	& 33.3 \\
        \hline
        LU & 90.1 &	90.7 & 96.3	& 78.0 & 100.0 & 78.0 & 46.1 & 45.0	& 76.1 \\
        \hline
        MG & 64.9 &	87.6 & 57.8	& 45.7 & 100.0 & 45.7 &	43.2 & 32.4	& 36.4 \\
        \hline
        SP & 88.8 &	94.7 & 92.2	& 83.4 & 100.0 & 83.4 &	46.6 & 46.4	& 81.7 \\
        \hline
        UA & 75.9 &	83.8 & 83.9	& 72.9 & 100.0 & 72.9 &	48.4 & 47.2	& 72.5 \\
        \hline
    \end{tabular}
    \caption{Predictive performance on different for different ML models measured with a k-fold CV method on the whole set of 1415 SNU NPB loops.}
    \label{tab:average_accuracy}
\end{table*}

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{figures/LOOCV_accuracy.pdf}
\caption{Prediction accuracy measured using k-fold CV on the whole SNU NPB loop set}
\label{fig:accuracy}
\end{figure}

\subsection{ICC competition scheme}
\label{evaluation_icc_competition}
\quad The first ML model practical use scenario is outlined in the section \ref{application_icc_competition}. In that scenario we harness ML model together with ICC compiler and provide a programmer with a feedback regarding loop parallelisability using outputs of both components. As our model has been trained with expert OpenMP labels along with regular ICC reports, it can potentially extend ICC capabilities in the loop parallelism discovery.\newline\null
\quad Before we ran that experiment we conducted a research into SNU NPB source code base and its parallelisation with ICC compiler. We identified all cases, where ICC misses parallel loops and classified the reasons into separate buckets. Table \ref{tab:icc_missed_opportunities} shows the results.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|p{2cm}|p{2cm}|p{2cm}|}
        \hline
        unrecognised reduction & array privatization & AA conservativeness \\
        \hline
        18 & 7 & 60 \\
        \hline
        unknown iteration number & static dependencies & too complex \\
        \hline
        7 & 46 & 22 \\
        \hline
        uninlined calls & other & total \\
        \hline
        4 & 4 & 168   \\
        \hline
    \end{tabular}
    \caption{Classification of loops missed by Intel Compiler for various reasons}
    \label{tab:icc_missed_opportunities}
\end{table}

After the study of ICC limitations on SNU NPB benchmarks we set the experiment to assess the performance of our scheme in the following way. We repeatedly ran k-fold CV on the whole set of SNU NPB loops and clustered them all into separate buckets according to the following combinations table .

Figure \ref{fig:icc_competition} shows the final distribution of all the cases as percentages of the total number of experiments for different ML models. As it can be seen from the figure in majority of cases our ML based tool agrees with ICC and either classifies truly non-parallel loops as non-parallel and truly parallel as parallel. We can explain this by the fact, that we trained our ML model with ICC optimization reports and to some degree our tool learnt ICC reasonings. However, sometimes there are disagreements between ICC and our tool. We can classify these disagreements into 3 buckets: the one, where our predictor makes a critical mistake by prediction non-parallel loop to be parallelisible; the one, where our predictor discovers a parallell loop missed by ICC.       

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/icc_competition.pdf}
\caption{Prediction accuracy measured using k-fold CV on the whole SNU NPB loop set}
\label{fig:icc_competition}
\end{figure*}

\subsection{SNU NAS Performance}
\label{evaluation_performance}
\quad This section reports on the assessment of software parallelisation assistant scheme proposed in the section \ref{application_profile_filter} of the paper. As the major fraction of program execution time is spent in program loops, they represent the most important program structure one should aim to parallelise. The ultimate goal of our \textit{smart} software parallelisation assistant is to ease the manual software parallelisation process by pointing out the program loops, where a programmer should concentrate his efforts. Following the order provided by our assistant a programmer should approach the best achievable performance levels faster then he would by following a profile-based loop order.\newline\null
\quad But before we can assess our assistant we need to conduct a study of performance we could potentially extract from SNU NPB benchmarks. To explore the potential present in SNU NPB benchmarks we measured their running times for differently compiled versions. 


Figure \ref{fig:snu_npb_performance} shows the results. It can be seen, that ICC compiler not only fails to achieve noticeable speedups, but actually significantly slows performance down on some of the benchmarks. While BT, CG, EP, FT, IS benchmarks show a good speedup for a manually parallelised benchmark developer version, the rest do not exhibit significant speedups with UA benchmark showing even a slowdown.\newline\null
\quad Our \textit{smart} parallelisation adviser reorders a runtime ordered list of benchmark loops in a way it thinks that minimises programmer parallelisation efforts and helps to close performance gap between a serial version and a parallel one. Our technique is not applicable to DC and IS benchmarks: these benchmarks get their parallel speedups from OpenMP parallel sections and not from parallel loops. Parallelisation of loops in these benchmarks (be it profile ordered list or the one of our smart adviser) actually slows them down. Figure \ref{fig:performance_convergence_line} shows performance convergence curves for the most interesting benchmarks.
\quad In order to deploy our adviser on SNU NPB benchmarks we have to use modified LOOCV methodology. We take the test benchmark and train our adviser on the 9 remaining ones. As section \label{loocv_accuracy} shows such a methodology has a lower predictive performance due to training set incompleteness, which potentially limits the success of our technique.   

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/benchmark_runtime.pdf}
\caption{SNU NPB running times for different compilation options: serial, automatic parallelisation and vectorization, OpenMP benchmark expert developer version and all the possible combinations of those.}
\label{fig:snu_npb_performance}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/bt_ft_cg_ua_parallelisation.pdf}
\caption{Example of performance convergence curves for the most representativ eBT, CG, FT, UA benchmarks. Our technique outperforms a simple profile based loop ranking for BT, CG and FT, but  }
\label{fig:performance_convergence_line}
\end{figure*}

\subsection{SNU NAS Performance}
\quad As the running time of a benchmark is ultimately the only true measure of parallelisation effectiveness on a given platform, we need a methodology to get these numbers. To estimate potential performance speedup our scheme might bring for every SNU NAS benchmark we use a modified LOOCV approach. Given feature vectors of SNU NAS benchmark loops and corresponding parallelisability labels we train our predictor on 9 remaining benchmarks and test it on the chosen one. For the chosen benchmark we get predictor's feedback        

\quad The common general thing about all SNU NAS benchmarks we observed is that the main portion of benchmark speedup comes from a coarse-grain manual parallelisation done by its developers. The main loops preceded by OpenMP pragmas are quite big and complex. Usually they contain calls to different functions, which do not always get inlined, as well as a lot of multidimensional arrays with complex index computations. Many array references happen indirectly and thus require runtime behaviour knowledge for their analysis. Parallelisation of such loops requires a thorough comprehension of the source code by a programmer, and is beyond the capabilities of the state-of-the-art automatic tools.\newline
\quad Intel compiler vectorises and parallelises quite a significant number of SNU NAS benchmark loops, but these loops are not the main ones. When we measure the performance of parallel and vector codes generated by Intel compiler, we observe running times being slightly better than in serial versions for vector codes and significant slowdowns for automatic ICC parallelisation.       


\quad The major weakness of Intel compiler, as well as our trained loop parallelisability classifier is that they mostly discover relatively fine-grain parallelism and miss opportunities seized by manual coarse-grain parallelisation. SNU NAS benchmarks contain a lot of big loops with deep nesting and uninlined function calls inside. SNU NAS developers have deep benchmark behaviour understanding and know exactly where even such loops can be parallelised, but that knowledge is far beyond the sight of automatic tools. 
\quad Parallelism present in EP and DC benchmarks is undetectable neither by Intel compiler nor by our oracle. DC benchmark contains only 1 parallel section encapsulating many function calls and complex control flow. Such parallelism cannot be detected in principle. Performance of EP benchmark depends heavily on the single loop with a reduction. SNU NPB developers have successfully manually parallelised this loop, but neither ICC nor trained oracle could find parallelism in it. Oracle predicted thsi loop to be 40\% parallelisible. The loop is complex and contains 2 inner loops, where reduction variables are buried as well as timer and random generator function calls.\newline
\quad Our scheme does not utilise all the coarse-grain parallelism of FT benchmark as well. Developers parallelise outermost loops of loop nests, whereas our classifier finds parallelism only in a modestly-sized inner loops, which do not contain function calls (thus operating at a finer level). Such finer parallelisation is not always beneficial and introduces overheads from implicit OpenMP barriers.
\quad Our scheme advised us to parallelise almost all BT benchmark loops, which contain OpenMP pragma in the original hand-parallelised benchmark version. There are a couple of missed OpenMP pragmas. Moreover, our scheme advises us to parallelise inner loops with a finer grains of parallelism, but doing so results in a slowdown, rather than speedup. So we take predictor's feedback and apply it on the top of our common sense that only outer loops should be parallelised to avoid synchronisation overheads and stalls.    

\quad Table illustrates the final SNU NPB parallelisation report.

\begin{table*}[h!]
    \centering
    \begin{tabular}[c]{|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}
        \hline
        \multirow{2}{2.5cm}{benchmark} & \multicolumn{3}{c}{benchmark running time, sec} \vline & \multicolumn{2}{c}{speedup, times} \vline & \multicolumn{6}{c}{loops number} \vline\\\cline{2-12}
        & serial & openmp & critical & openmp speedup & critical speedup & profile & svc & mlp & rfc & ada & dt\\\cline{1-12}
        \hline
        BT & 158.76 & 57.36 & 56.57 & 2.77 & 2.81 & 6 & \cellcolor[HTML]{FA8D8D} 8 & \cellcolor[HTML]{7BB66B} 4 & \cellcolor[HTML]{7BB66B} 5 & \cellcolor[HTML]{7BB66B} 3 & \cellcolor[HTML]{7BB66B} 5\\
        \hline
        CG & 69.38 & 19.77 & 25.06 & 3.51 & 2.77 & 3 & \cellcolor[HTML]{7BB66B} 2 & \cellcolor[HTML]{7BB66B} 1 & \cellcolor[HTML]{7BB66B} 1 & \cellcolor[HTML]{7BB66B} 1 & \cellcolor[HTML]{7BB66B} 1\\
        \hline
        DC & 698.82 & 254.29 & 698.82 & 2.75 & 1.00 & 0 & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf\\
        \hline
        EP & 86.35 & 35.40 & 35.07 & 2.44 & 2.46 & 1 & \cellcolor[HTML]{91A1FA} 1 & \cellcolor[HTML]{91A1FA} 1 & \cellcolor[HTML]{91A1FA} 1 & \cellcolor[HTML]{91A1FA} 1 & \cellcolor[HTML]{91A1FA} 1\\
        \hline
        FT & 36.81 & 12.13 & 14.69 & 3.03 & 2.51 & 9 & \cellcolor[HTML]{7BB66B} 4 & \cellcolor[HTML]{7BB66B} 3 & \cellcolor[HTML]{7BB66B} 4 & \cellcolor[HTML]{91A1FA} 9 & \cellcolor[HTML]{7BB66B} 5\\
        \hline
        IS & 4.75 & 1.35 & 4.63 & 3.53 & 1.03 & inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf\\
        \hline
        LU & 115.46 & 55.00 & 140.53 & 2.10 & 0.82 & inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf & \cellcolor[HTML]{91A1FA} inf\\
        \hline
        MG & 5.20 & 3.58 & 3.94 & 1.45 & 1.32 & 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{91A1FA} 3\\
        \hline
        SP & 86.65 & 65.19 & 62.90 & 1.33 & 1.38 & 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{FA8D8D} inf & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{91A1FA} 3 & \cellcolor[HTML]{FA8D8D} 20\\
        \hline
        UA & 71.82 & 78.56 & 189.66 & 0.91 & 0.38 & 19 & \cellcolor[HTML]{FA8D8D} 30 & \cellcolor[HTML]{FA8D8D} 30 & \cellcolor[HTML]{91A1FA} 19 & \cellcolor[HTML]{FA8D8D} 22 & \cellcolor[HTML]{7BB66B} 10\\
        \hline
    \end{tabular}
    \caption{Final SNU NPB manual paralelization report. Running times and corresponding speedups are provided for \textit{serial}, \textit{OpenMP} and \textit{critical loops only} benchmark versions. OpenMP is the hand-parallelized version of SNU NPB developers. Critical  }
    \label{tab:final_parallel_table}
\end{table*}

\section{Related Work}
\label{related_work}

\subsection{Loop Parallelisation}

\subsection{Machine Learning in Compilers}
\quad Correctness is critical

it can be used for topics ranging from selecting the best compiler flags to determining how to map parallelism to processors.

tradition in computer science in increasing automation

It also brings compilation nearer to the standards of evidence based science.

It introduces an experimental methodology where we separate out evaluation from design and considers the robustness of solutions.

Next, in Section V, we review how previous work chooses quantifiable properties, or features, to represent programs.

\subsection{Machine Learning and Parallelisation}



Machine learning techniques have already been applied in the field of compilers, but their application is mostly concentrated in the task of auto-tuning and optimal mapping of software onto a range of diverse hardware platforms. That fact has a reasonable explanaition. Compiler has to produce semantically correct translation of high-level source program into the instructions of a target machine. Statistical nature of machine learning approaches and their inherent errors limit their application in the field of compilers. Machine learning approaches has been applied to the tasks of selecting the best compiler flags for mapping software onto different kinds of hardware. This areas are safe for machine learning application. 

In this work on the contrary we step into potentially dangerous machine learning application area. We investigate the task of loop parallelisability prediction. We calculate the number of unsafe error cases and propose a scheme ensuring the final correctness of produced parallel software.

This is not the only effort of extracting the best performance out of software potentially compromising its correctness. There is a vast body of research of supplementing compiler static analyses with dynamic information. These techniques have a property that they cannot generally prove correctness of profile-guided transformations for the whole range of possible program inputs.

Work \cite{fried_ea:2013:icmla} is similar to ours, but it has a number of differences.  



\section{Summary \& Conclusions}

The ultimate goal of this work has always been to provide a programmer with a feedback on the parallelisability of software in general and loops in particular. The work has evolved from parallelisability correlations of single metrics (features) to a machine learning based tool for filtering runtime program profile and providing the resulting loop ranking for a programmer.\newline\null
\quad The ultimate performance of the tool depends on many factors. First 

\subsection{Future Work}

Having received some motivating results, there are still certain areas of improvement. The tool consists of a number of co-designed parts. Their tuning for maximum overall performance can be done infinitely. The ultimate performance of the tool is affected by a number of factors. The main factor is the nature of the programs (benchmarks) being used for ML training and testing stages. SNU NAS benchmarks are quite diverse in terms of the loops they contain. Loops have different sizes, nesting structures, parallelism patterns.

\quad We believe, that our idea can be worked on and improved even further. There are several possible potential steps.

\quad Our scheme consists of a number of components and steps. First we select training and testing programs, then we engineer the set of representative ML features. After 

\quad First, SNU NPB benchmarks have certain features and properties that reveal themselves in our work. 
\quad In our work we use SNU NAS Parallel Benchmarks for assessment of our machine learning based technique. Majority of SNU NPB benchmarks concentrate their running time in a relatively small set of critical loops. That fact presents a problem for a full scale assessment of our technique.      



Our tool consists of a lot of components to be tuned. The tuning work can be done infinetely in the attempt to get the best possible performance on a wider range of programs and benchmarks.    

\subsection{Future Work}





\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}